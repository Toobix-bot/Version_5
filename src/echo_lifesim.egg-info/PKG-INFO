Metadata-Version: 2.4
Name: echo-lifesim
Version: 0.1.0
Summary: ECHO-LifeSim MVP: Mirror-mode persona simulation with episodic memory, needs, and A/B action suggestions.
Author: You
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: pydantic>=2.7.0
Requires-Dist: httpx>=0.27.0
Requires-Dist: rich>=13.7.0
Requires-Dist: typer>=0.12.3
Requires-Dist: orjson>=3.10.0
Requires-Dist: python-dotenv>=1.0.0
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Provides-Extra: gui
Requires-Dist: streamlit>=1.36.0; extra == "gui"

# ECHO-LifeSim MVP

Minimaler Prototyp basierend auf deinem Konzept (Mirror-Mode, Episoden, Bedürfnisse, A/B Aktionen, Reflexion alle 5 Züge).

> NEU: Siehe `README_TUTORIAL.md` für einen geführten Einstieg (Erste Schritte, Beispiele, GUI, Tipps).

## Installation

Voraussetzung: Python 3.11+

```bash
pip install -e .[dev]
```

## CLI Nutzung

Eine Runde (User-Eingabe + optionales Event):
```bash
echo-sim turn "Ich bin müde aber will etwas Fokus finden" --event regen
```

Aktion anwenden (Label exakt übernehmen):
```bash
echo-sim act "2-Min atemfokus"
```

State ansehen:
```bash
echo-sim state
```

State speichern / laden / exportieren / reset:
```bash
echo-sim save
echo-sim load
echo-sim export export.json
echo-sim reset --confirm
```

Groq API Key (Windows PowerShell Beispiel):
```powershell
setx GROQ_API_KEY "DEIN_KEY_HIER"
```
Danach neues Terminal öffnen. Optional `.env` Datei anlegen:
```
GROQ_API_KEY=DEIN_KEY_HIER
GROQ_MODEL=llama-3.1-8b-instant
GROQ_MODELS=llama-3.1-8b-instant,llama-3.1-70b-versatile,mixtral-8x7b-32768
```
Siehe `.env.example` als Vorlage. Datei `.env` wird automatisch via `python-dotenv` geladen.

Modelle anzeigen / setzen:
```bash
echo-sim llm-models
echo-sim set-model llama-3.1-70b-versatile
```

Hinweise (Kurzcharakteristik):
- llama-3.1-8b-instant: schnell & günstig, Standard für kurze Antworten
- llama-3.1-70b-versatile: höherer Kontext & Stilnuancen
- mixtral-8x7b-32768: längerer Kontext, gute Balance

## Konzept-Features (MVP enthalten)

## Geplante Erweiterungen

## Architektur Überblick
- models.py: Pydantic Datamodelle (State, Needs, Episodes)
- memory.py: Heuristisches Retrieval
- engine.py: Core Loop (ingest → event → retrieve → reply → reflection)
- cli.py: Typer basiertes Interface

## Designprinzipien
1. Erklärbarkeit vor Komplexität
2. Keine versteckten Mutationen – alles im State sichtbar
3. Kurze, konkrete, warme Antworten (kein Kitsch)
4. Balance statt Maximierung (Needs normalisieren Richtung 50)
5. Erweiterbarkeit (Overmind, Buffs, Skills modular anschließbar)

## Nächste konkrete Schritte
1. llm_client.py für Groq API (Konfig via ENV: GROQ_API_KEY)
2. Persistenz (State als JSON speichern/laden)
3. Buff/Debuff Schema + Anwendung pro Turn
4. Tests (Pytest) für Retrieval & Need-Updates
5. Export/Reset Kommandos

## Sicherheit / Guardrails (geplant)
- Kennzeichnung als Simulation
- Sensible Themen: neutrale, unterstützende Sprache + Hinweis-Ressourcen (konfigurierbar)
- Offline-first, klarer Opt-in für externe Calls

---
Feedback willkommen – wir können nun gezielt Module vertiefen (LLM Integration, Overmind, Buff-System). Sag einfach, welches Teil als nächstes dran ist.

